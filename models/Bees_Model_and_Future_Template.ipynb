{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rtFKCS4LkOzN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pRc8O8CvkYx1"
   },
   "outputs": [],
   "source": [
    "years = range(2017,2022)\n",
    "naics_level = [2,4,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IdUt24w63WDa"
   },
   "outputs": [],
   "source": [
    "state_dict = {\n",
    "    \"AL\": \"ALABAMA\",\n",
    "    \"AK\": \"ALASKA\",\n",
    "    \"AZ\": \"ARIZONA\",\n",
    "    \"AR\": \"ARKANSAS\",\n",
    "    \"CA\": \"CALIFORNIA\",\n",
    "    \"CO\": \"COLORADO\",\n",
    "    \"CT\": \"CONNECTICUT\",\n",
    "    \"DE\": \"DELAWARE\",\n",
    "    \"FL\": \"FLORIDA\",\n",
    "    \"GA\": \"GEORGIA\",\n",
    "    \"HI\": \"HAWAII\",\n",
    "    \"ID\": \"IDAHO\",\n",
    "    \"IL\": \"ILLINOIS\",\n",
    "    \"IN\": \"INDIANA\",\n",
    "    \"IA\": \"IOWA\",\n",
    "    \"KS\": \"KANSAS\",\n",
    "    \"KY\": \"KENTUCKY\",\n",
    "    \"LA\": \"LOUISIANA\",\n",
    "    \"ME\": \"MAINE\",\n",
    "    \"MD\": \"MARYLAND\",\n",
    "    \"MA\": \"MASSACHUSETTS\",\n",
    "    \"MI\": \"MICHIGAN\",\n",
    "    \"MN\": \"MINNESOTA\",\n",
    "    \"MS\": \"MISSISSIPPI\",\n",
    "    \"MO\": \"MISSOURI\",\n",
    "    \"MT\": \"MONTANA\",\n",
    "    \"NE\": \"NEBRASKA\",\n",
    "    \"NV\": \"NEVADA\",\n",
    "    \"NH\": \"NEW HAMPSHIRE\",\n",
    "    \"NJ\": \"NEW JERSEY\",\n",
    "    \"NM\": \"NEW MEXICO\",\n",
    "    \"NY\": \"NEW YORK\",\n",
    "    \"NC\": \"NORTH CAROLINA\",\n",
    "    \"ND\": \"NORTH DAKOTA\",\n",
    "    \"OH\": \"OHIO\",\n",
    "    \"OK\": \"OKLAHOMA\",\n",
    "    \"OR\": \"OREGON\",\n",
    "    \"PA\": \"PENNSYLVANIA\",\n",
    "    \"RI\": \"RHODE ISLAND\",\n",
    "    \"SC\": \"SOUTH CAROLINA\",\n",
    "    \"SD\": \"SOUTH DAKOTA\",\n",
    "    \"TN\": \"TENNESSEE\",\n",
    "    \"TX\": \"TEXAS\",\n",
    "    \"UT\": \"UTAH\",\n",
    "    \"VT\": \"VERMONT\",\n",
    "    \"VA\": \"VIRGINIA\",\n",
    "    \"WA\": \"WASHINGTON\",\n",
    "    \"WV\": \"WEST VIRGINIA\",\n",
    "    \"WI\": \"WISCONSIN\",\n",
    "    \"WY\": \"WYOMING\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KWtFXA8e2RC_"
   },
   "outputs": [],
   "source": [
    "target_url = f\"https://raw.githubusercontent.com/ModelEarth/RealityStream/main/input/bees/targets/bees-targets.csv\"\n",
    "target_df = pd.read_csv(target_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jv_AUQwjnrkN"
   },
   "outputs": [],
   "source": [
    "def rename_columns(df, year):\n",
    "    rename_mapping = {}\n",
    "    for column in df.columns:\n",
    "      if column not in df.columns[:2]:\n",
    "          new_column_name = column + f'-{year}'\n",
    "          rename_mapping[column] = new_column_name\n",
    "\n",
    "    df.rename(columns=rename_mapping, inplace=True)\n",
    "\n",
    "def check_directory(directory_path):\n",
    "    if not os.path.exists(directory_path):\n",
    "        try:\n",
    "            os.makedirs(directory_path)\n",
    "            print(f\"Directory '{directory_path}' created successfully.\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating directory '{directory_path}': {e}\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory_path}' already exists.\")\n",
    "    return directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kd5ZXB8M2Sp6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '../process/bees/states-2022_increase-bees' created successfully.\n"
     ]
    }
   ],
   "source": [
    "target_column = '2022_increase'\n",
    "target_list = ['2007_increase','2012_increase','2017_increase','2022_increase']\n",
    "target_list.remove(target_column)\n",
    "year_list = [\"2002\",\"2007\",\"2012\",\"2017\",\"2022\"]\n",
    "\n",
    "save_dir = f\"../process/bees/states-{target_column}-bees\"\n",
    "check_directory(save_dir)\n",
    "\n",
    "for state in state_dict:\n",
    "    data = {}\n",
    "    for year in years:\n",
    "        url = f\"https://raw.githubusercontent.com/ModelEarth/community-timelines/main/training/naics2/US/counties/{year}/US-{state}-training-naics2-counties-{year}.csv\"\n",
    "        data[year] = pd.read_csv(url)\n",
    "        rename_columns(data[year], year)\n",
    "\n",
    "    merged_df_feature = pd.merge(data[2017], data[2018], on=['Fips', 'Name'], how='inner')\n",
    "\n",
    "    for year in range(2019,2022):\n",
    "        merged_df_feature = pd.merge(merged_df_feature, data[year], on=['Fips', 'Name'], how='inner')\n",
    "\n",
    "    cols = merged_df_feature.columns.tolist()\n",
    "    cols = cols[:2] + sorted(cols[2:])\n",
    "    merged_df_feature = merged_df_feature[cols].rename(columns={\"Name\": \"County\"})\n",
    "    \n",
    "    merged_df = pd.merge(merged_df_feature, target_df[target_df[\"State\"]==state_dict[state]], on=[\"Fips\",\"County\"], how=\"inner\")\n",
    "    merged_df.drop(columns=['Unnamed: 0','Name','State','State ANSI', 'County ANSI', \"Ag District\", \"Ag District Code\"] + target_list + year_list, axis=1, inplace=True)\n",
    "\n",
    "    target = merged_df.iloc[:, -1]\n",
    "    merged_df.drop(columns=[target_column], axis=1, inplace=True)\n",
    "    merged_df.insert(0, 'target', target)\n",
    "\n",
    "    merged_df.to_csv(os.path.join(save_dir, f\"{state}-{target_column}-bees.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '../process/bees/states-2007_increase-bees' created successfully.\n"
     ]
    }
   ],
   "source": [
    "target_column = '2007_increase'\n",
    "target_list = ['2007_increase','2012_increase','2017_increase','2022_increase']\n",
    "target_list.remove(target_column)\n",
    "year_list = [\"2002\",\"2007\",\"2012\",\"2017\",\"2022\"]\n",
    "\n",
    "save_dir = f\"../process/bees/states-{target_column}-bees\"\n",
    "check_directory(save_dir)\n",
    "\n",
    "for state in state_dict:\n",
    "    data = {}\n",
    "    for year in years:\n",
    "        url = f\"https://raw.githubusercontent.com/ModelEarth/community-timelines/main/training/naics2/US/counties/{year}/US-{state}-training-naics2-counties-{year}.csv\"\n",
    "        data[year] = pd.read_csv(url)\n",
    "        rename_columns(data[year], year)\n",
    "\n",
    "    merged_df_feature = pd.merge(data[2017], data[2018], on=['Fips', 'Name'], how='inner')\n",
    "\n",
    "    for year in range(2019,2022):\n",
    "        merged_df_feature = pd.merge(merged_df_feature, data[year], on=['Fips', 'Name'], how='inner')\n",
    "\n",
    "    cols = merged_df_feature.columns.tolist()\n",
    "    cols = cols[:2] + sorted(cols[2:])\n",
    "    merged_df_feature = merged_df_feature[cols].rename(columns={\"Name\": \"County\"})\n",
    "    \n",
    "    merged_df = pd.merge(merged_df_feature, target_df[target_df[\"State\"]==state_dict[state]], on=[\"Fips\",\"County\"], how=\"inner\")\n",
    "    merged_df.drop(columns=['Unnamed: 0','Name','State','State ANSI', 'County ANSI', \"Ag District\", \"Ag District Code\"] + target_list + year_list, axis=1, inplace=True)\n",
    "\n",
    "    target = merged_df.iloc[:, -1]\n",
    "    merged_df.drop(columns=[target_column], axis=1, inplace=True)\n",
    "    merged_df.insert(0, 'target', target)\n",
    "\n",
    "    merged_df.to_csv(os.path.join(save_dir, f\"{state}-{target_column}-bees.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '../process/bees/states-2012_increase-bees' created successfully.\n"
     ]
    }
   ],
   "source": [
    "target_column = '2012_increase'\n",
    "target_list = ['2007_increase','2012_increase','2017_increase','2022_increase']\n",
    "target_list.remove(target_column)\n",
    "year_list = [\"2002\",\"2007\",\"2012\",\"2017\",\"2022\"]\n",
    "\n",
    "save_dir = f\"../process/bees/states-{target_column}-bees\"\n",
    "check_directory(save_dir)\n",
    "\n",
    "for state in state_dict:\n",
    "    data = {}\n",
    "    for year in years:\n",
    "        url = f\"https://raw.githubusercontent.com/ModelEarth/community-timelines/main/training/naics2/US/counties/{year}/US-{state}-training-naics2-counties-{year}.csv\"\n",
    "        data[year] = pd.read_csv(url)\n",
    "        rename_columns(data[year], year)\n",
    "\n",
    "    merged_df_feature = pd.merge(data[2017], data[2018], on=['Fips', 'Name'], how='inner')\n",
    "\n",
    "    for year in range(2019,2022):\n",
    "        merged_df_feature = pd.merge(merged_df_feature, data[year], on=['Fips', 'Name'], how='inner')\n",
    "\n",
    "    cols = merged_df_feature.columns.tolist()\n",
    "    cols = cols[:2] + sorted(cols[2:])\n",
    "    merged_df_feature = merged_df_feature[cols].rename(columns={\"Name\": \"County\"})\n",
    "    \n",
    "    merged_df = pd.merge(merged_df_feature, target_df[target_df[\"State\"]==state_dict[state]], on=[\"Fips\",\"County\"], how=\"inner\")\n",
    "    merged_df.drop(columns=['Unnamed: 0','Name','State','State ANSI', 'County ANSI', \"Ag District\", \"Ag District Code\"] + target_list + year_list, axis=1, inplace=True)\n",
    "\n",
    "    target = merged_df.iloc[:, -1]\n",
    "    merged_df.drop(columns=[target_column], axis=1, inplace=True)\n",
    "    merged_df.insert(0, 'target', target)\n",
    "\n",
    "    merged_df.to_csv(os.path.join(save_dir, f\"{state}-{target_column}-bees.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '../process/bees/states-2017_increase-bees' already exists.\n"
     ]
    }
   ],
   "source": [
    "target_column = '2017_increase'\n",
    "target_list = ['2007_increase','2012_increase','2017_increase','2022_increase']\n",
    "target_list.remove(target_column)\n",
    "year_list = [\"2002\",\"2007\",\"2012\",\"2017\",\"2022\"]\n",
    "\n",
    "save_dir = f\"../process/bees/states-{target_column}-bees\"\n",
    "check_directory(save_dir)\n",
    "\n",
    "for state in state_dict:\n",
    "    data = {}\n",
    "    for year in years:\n",
    "        url = f\"https://raw.githubusercontent.com/ModelEarth/community-timelines/main/training/naics2/US/counties/{year}/US-{state}-training-naics2-counties-{year}.csv\"\n",
    "        data[year] = pd.read_csv(url)\n",
    "        rename_columns(data[year], year)\n",
    "\n",
    "    merged_df_feature = pd.merge(data[2017], data[2018], on=['Fips', 'Name'], how='inner')\n",
    "\n",
    "    for year in range(2019,2022):\n",
    "        merged_df_feature = pd.merge(merged_df_feature, data[year], on=['Fips', 'Name'], how='inner')\n",
    "\n",
    "    cols = merged_df_feature.columns.tolist()\n",
    "    cols = cols[:2] + sorted(cols[2:])\n",
    "    merged_df_feature = merged_df_feature[cols].rename(columns={\"Name\": \"County\"})\n",
    "    \n",
    "    merged_df = pd.merge(merged_df_feature, target_df[target_df[\"State\"]==state_dict[state]], on=[\"Fips\",\"County\"], how=\"inner\")\n",
    "    merged_df.drop(columns=['Unnamed: 0','Name','State','State ANSI', 'County ANSI', \"Ag District\", \"Ag District Code\"] + target_list + year_list, axis=1, inplace=True)\n",
    "\n",
    "    target = merged_df.iloc[:, -1]\n",
    "    merged_df.drop(columns=[target_column], axis=1, inplace=True)\n",
    "    merged_df.insert(0, 'target', target)\n",
    "\n",
    "    merged_df.to_csv(os.path.join(save_dir, f\"{state}-{target_column}-bees.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "PoHLYF61xyl1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '../process/bees/states-2017_increase-bees' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Honglin Zhu\\AppData\\Local\\Temp\\ipykernel_14428\\4271121803.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  integrated_df = pd.concat(dataframes, ignore_index=True)\n",
      "C:\\Users\\Honglin Zhu\\AppData\\Local\\Temp\\ipykernel_14428\\4271121803.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  integrated_df = pd.concat(dataframes, ignore_index=True)\n",
      "C:\\Users\\Honglin Zhu\\AppData\\Local\\Temp\\ipykernel_14428\\4271121803.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  integrated_df = pd.concat(dataframes, ignore_index=True)\n",
      "C:\\Users\\Honglin Zhu\\AppData\\Local\\Temp\\ipykernel_14428\\4271121803.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  integrated_df = pd.concat(dataframes, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "target_list = ['2007_increase','2012_increase','2017_increase','2022_increase']\n",
    "full_save_dir = f\"../output/bees/training\"\n",
    "check_directory(save_dir)\n",
    "for current_target_column in target_list:\n",
    "    dataframes = []\n",
    "    \n",
    "    csv_directory = f\"../process/bees/states-{current_target_column}-bees\"\n",
    "    csv_files = os.listdir(csv_directory)\n",
    "    for csv_file in csv_files:\n",
    "        if csv_file.endswith('.csv'):\n",
    "            dataframes.append(pd.read_csv(os.path.join(csv_directory, csv_file)))\n",
    "    \n",
    "    integrated_df = pd.concat(dataframes, ignore_index=True)\n",
    "    integrated_df.to_csv(os.path.join(full_save_dir, f\"full-{current_target_column}-bees.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>Fips</th>\n",
       "      <th>County</th>\n",
       "      <th>Emp-11-2017</th>\n",
       "      <th>Emp-11-2018</th>\n",
       "      <th>Emp-11-2019</th>\n",
       "      <th>Emp-11-2020</th>\n",
       "      <th>Emp-11-2021</th>\n",
       "      <th>Emp-21-2017</th>\n",
       "      <th>Emp-21-2018</th>\n",
       "      <th>...</th>\n",
       "      <th>Population-2017</th>\n",
       "      <th>Population-2018</th>\n",
       "      <th>Population-2019</th>\n",
       "      <th>Population-2020</th>\n",
       "      <th>Population-2021</th>\n",
       "      <th>UrbanDensity-2017</th>\n",
       "      <th>UrbanDensity-2018</th>\n",
       "      <th>UrbanDensity-2019</th>\n",
       "      <th>UrbanDensity-2020</th>\n",
       "      <th>UrbanDensity-2021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1099</td>\n",
       "      <td>Monroe County</td>\n",
       "      <td>207.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1079</td>\n",
       "      <td>Lawrence County</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1081</td>\n",
       "      <td>Lee County</td>\n",
       "      <td>79.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>157.0</td>\n",
       "      <td>159</td>\n",
       "      <td>161</td>\n",
       "      <td>163</td>\n",
       "      <td>172.0</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.68</td>\n",
       "      <td>2.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1093</td>\n",
       "      <td>Marion County</td>\n",
       "      <td>71.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1107</td>\n",
       "      <td>Pickens County</td>\n",
       "      <td>89.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8545</th>\n",
       "      <td>1.0</td>\n",
       "      <td>56017</td>\n",
       "      <td>Hot Springs County</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>244.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8546</th>\n",
       "      <td>0.0</td>\n",
       "      <td>56013</td>\n",
       "      <td>Fremont County</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>467.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8547</th>\n",
       "      <td>1.0</td>\n",
       "      <td>56035</td>\n",
       "      <td>Sublette County</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>843.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8548</th>\n",
       "      <td>1.0</td>\n",
       "      <td>56045</td>\n",
       "      <td>Weston County</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8549</th>\n",
       "      <td>1.0</td>\n",
       "      <td>56031</td>\n",
       "      <td>Platte County</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8550 rows × 288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target   Fips              County  Emp-11-2017  Emp-11-2018  \\\n",
       "0        0.0   1099       Monroe County        207.0        217.0   \n",
       "1        0.0   1079     Lawrence County          NaN          NaN   \n",
       "2        1.0   1081          Lee County         79.0         83.0   \n",
       "3        1.0   1093       Marion County         71.0         68.0   \n",
       "4        0.0   1107      Pickens County         89.0         95.0   \n",
       "...      ...    ...                 ...          ...          ...   \n",
       "8545     1.0  56017  Hot Springs County          NaN          NaN   \n",
       "8546     0.0  56013      Fremont County          2.0          4.0   \n",
       "8547     1.0  56035     Sublette County          3.0          4.0   \n",
       "8548     1.0  56045       Weston County          NaN          NaN   \n",
       "8549     1.0  56031       Platte County          4.0          NaN   \n",
       "\n",
       "      Emp-11-2019  Emp-11-2020  Emp-11-2021  Emp-21-2017  Emp-21-2018  ...  \\\n",
       "0           192.0        174.0        200.0          NaN          NaN  ...   \n",
       "1             NaN          NaN          NaN          NaN          NaN  ...   \n",
       "2            43.0         70.0         75.0          NaN          NaN  ...   \n",
       "3            63.0         56.0         54.0          NaN          NaN  ...   \n",
       "4            92.0         79.0         70.0          NaN          NaN  ...   \n",
       "...           ...          ...          ...          ...          ...  ...   \n",
       "8545          NaN          NaN          NaN        244.0        319.0  ...   \n",
       "8546          3.0          3.0          2.0        489.0        467.0  ...   \n",
       "8547          4.0          2.0          3.0        843.0        998.0  ...   \n",
       "8548          NaN          NaN          NaN         70.0         73.0  ...   \n",
       "8549          5.0          6.0          5.0          NaN          NaN  ...   \n",
       "\n",
       "      Population-2017  Population-2018  Population-2019  Population-2020  \\\n",
       "0                22.0               22               21               21   \n",
       "1                33.0               33               33               33   \n",
       "2               157.0              159              161              163   \n",
       "3                30.0               30               30               30   \n",
       "4                20.0               20               20               20   \n",
       "...               ...              ...              ...              ...   \n",
       "8545              5.0                5                5                5   \n",
       "8546             40.0               40               40               40   \n",
       "8547             10.0               10               10               10   \n",
       "8548              7.0                7                7                7   \n",
       "8549              9.0                9                9                9   \n",
       "\n",
       "      Population-2021  UrbanDensity-2017  UrbanDensity-2018  \\\n",
       "0                20.0               0.00               0.00   \n",
       "1                33.0               0.00               0.00   \n",
       "2               172.0               2.36               2.17   \n",
       "3                29.0               0.00               0.00   \n",
       "4                19.0               0.00               0.00   \n",
       "...               ...                ...                ...   \n",
       "8545              5.0               0.00               0.00   \n",
       "8546             39.0               1.81               1.70   \n",
       "8547              9.0               0.00               0.00   \n",
       "8548              7.0               0.00               0.00   \n",
       "8549              9.0               0.00               0.00   \n",
       "\n",
       "      UrbanDensity-2019  UrbanDensity-2020  UrbanDensity-2021  \n",
       "0                  0.00               0.00               0.00  \n",
       "1                  0.00               0.00               0.00  \n",
       "2                  2.11               2.68               2.67  \n",
       "3                  0.00               0.00               0.00  \n",
       "4                  0.00               0.00               0.00  \n",
       "...                 ...                ...                ...  \n",
       "8545               0.00               0.00               0.00  \n",
       "8546               1.72               1.53               1.63  \n",
       "8547               0.00               0.00               0.00  \n",
       "8548               0.00               0.00               0.00  \n",
       "8549               0.00               0.00               0.00  \n",
       "\n",
       "[8550 rows x 288 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integrated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, over_sample):\n",
    "    if over_sample:\n",
    "        sm = SMOTE(random_state = 2)\n",
    "        X_train, y_train = sm.fit_resample(X_train, y_train.ravel())\n",
    "        print(\"Oversampling Done for Training Data.\")\n",
    "\n",
    "    model = model.fit(X_train, y_train)\n",
    "    print(\"Model Fitted Successfully.\")\n",
    "\n",
    "    # calculating y_pred\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_prob = model.predict_proba(X_test)\n",
    "    roc_auc = round(roc_auc_score(y_test, y_pred_prob[:, 1]), 2)\n",
    "\n",
    "    print(f\"\\033[1mROC-AUC Score\\033[0m \\t\\t: {roc_auc*100} %\")\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:,1], pos_label=1)\n",
    "    \n",
    "    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "    \n",
    "    ix = np.argmax(gmeans)\n",
    "    print('\\033[1mBest Threshold\\033[0m \\t\\t: %.3f \\n\\033[1mG-Mean\\033[0m \\t\\t\\t: %.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "    y_pred = (y_pred > thresholds[ix])\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"\\033[1mModel Accuracy\\033[0m \\t\\t:\", round(accuracy,2,)*100, \"%\")\n",
    "\n",
    "    print(\"\\033[1m\\nClassification Report:\\033[0m\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return model, y_pred\n",
    "\n",
    "def save_model(model, dataset_name, model_name):\n",
    "    data = {\n",
    "    \"model\": model\n",
    "    }\n",
    "    with open(f'../output/{dataset_name}/saved/trained_{model_name}.pkl', 'wb') as file:\n",
    "        pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022_increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_target_column = \"2022_increase\"\n",
    "integrated_df = pd.read_csv(os.path.join(f\"../output/bees/training\",f\"full-{current_target_column}-bees.csv\"))\n",
    "X_total, y_total = integrated_df.iloc[:, 3:], integrated_df.iloc[:, 0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, test_size=0.2, random_state=42)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted Successfully.\n",
      "\u001b[1mROC-AUC Score\u001b[0m \t\t: 47.0 %\n",
      "\u001b[1mBest Threshold\u001b[0m \t\t: 0.651 \n",
      "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.498\n",
      "\u001b[1mModel Accuracy\u001b[0m \t\t: 57.99999999999999 %\n",
      "\u001b[1m\n",
      "Classification Report:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.27      0.10      0.14       204\n",
      "         1.0       0.63      0.85      0.72       366\n",
      "\n",
      "    accuracy                           0.58       570\n",
      "   macro avg       0.45      0.47      0.43       570\n",
      "weighted avg       0.50      0.58      0.52       570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_LR = LogisticRegression(max_iter=10000)\n",
    "model_LR, y_pred = train_model(model_LR, X_train_imputed, y_train, X_test_imputed, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted Successfully.\n",
      "\u001b[1mROC-AUC Score\u001b[0m \t\t: 53.0 %\n",
      "\u001b[1mBest Threshold\u001b[0m \t\t: 0.650 \n",
      "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.513\n",
      "\u001b[1mModel Accuracy\u001b[0m \t\t: 62.0 %\n",
      "\u001b[1m\n",
      "Classification Report:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.14      0.21       204\n",
      "         1.0       0.65      0.89      0.75       366\n",
      "\n",
      "    accuracy                           0.62       570\n",
      "   macro avg       0.53      0.51      0.48       570\n",
      "weighted avg       0.56      0.62      0.55       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_RF = RandomForestClassifier(n_jobs=3, oob_score=True, n_estimators=100, criterion=\"gini\")\n",
    "model_RF, y_pred = train_model(model_RF, X_train_imputed, y_train, X_test_imputed, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted Successfully.\n",
      "\u001b[1mROC-AUC Score\u001b[0m \t\t: 47.0 %\n",
      "\u001b[1mBest Threshold\u001b[0m \t\t: 0.651 \n",
      "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.498\n",
      "\u001b[1mModel Accuracy\u001b[0m \t\t: 57.99999999999999 %\n",
      "\u001b[1m\n",
      "Classification Report:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.27      0.10      0.14       204\n",
      "         1.0       0.63      0.85      0.72       366\n",
      "\n",
      "    accuracy                           0.58       570\n",
      "   macro avg       0.45      0.47      0.43       570\n",
      "weighted avg       0.50      0.58      0.52       570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_SVM = SVC()\n",
    "model_SVM, y_pred = train_model(model_LR, X_train_imputed, y_train, X_test_imputed, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted Successfully.\n",
      "\u001b[1mROC-AUC Score\u001b[0m \t\t: 51.0 %\n",
      "\u001b[1mBest Threshold\u001b[0m \t\t: 1.000 \n",
      "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.476\n",
      "\u001b[1mModel Accuracy\u001b[0m \t\t: 56.00000000000001 %\n",
      "\u001b[1m\n",
      "Classification Report:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.37      0.32      0.34       204\n",
      "         1.0       0.65      0.70      0.67       366\n",
      "\n",
      "    accuracy                           0.56       570\n",
      "   macro avg       0.51      0.51      0.51       570\n",
      "weighted avg       0.55      0.56      0.56       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "model_mlp = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "model_mlp, y_pred = train_model(model_mlp, X_train_imputed, y_train, X_test_imputed, y_test, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2017_increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_target_column = \"2017_increase\"\n",
    "integrated_df = pd.read_csv(os.path.join(f\"../output/bees/training\",f\"full-{current_target_column}-bees.csv\"))\n",
    "X_total, y_total = integrated_df.iloc[:, 3:], integrated_df.iloc[:, 0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, test_size=0.2, random_state=42)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted Successfully.\n",
      "\u001b[1mROC-AUC Score\u001b[0m \t\t: 60.0 %\n",
      "\u001b[1mBest Threshold\u001b[0m \t\t: 0.507 \n",
      "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.586\n",
      "\u001b[1mModel Accuracy\u001b[0m \t\t: 56.99999999999999 %\n",
      "\u001b[1m\n",
      "Classification Report:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.53      0.58       317\n",
      "         1.0       0.52      0.62      0.57       253\n",
      "\n",
      "    accuracy                           0.57       570\n",
      "   macro avg       0.58      0.58      0.57       570\n",
      "weighted avg       0.59      0.57      0.57       570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_LR = LogisticRegression(max_iter=10000)\n",
    "model_LR, y_pred = train_model(model_LR, X_train_imputed, y_train, X_test_imputed, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted Successfully.\n",
      "\u001b[1mROC-AUC Score\u001b[0m \t\t: 69.0 %\n",
      "\u001b[1mBest Threshold\u001b[0m \t\t: 0.480 \n",
      "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.643\n",
      "\u001b[1mModel Accuracy\u001b[0m \t\t: 63.0 %\n",
      "\u001b[1m\n",
      "Classification Report:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.55      0.62       317\n",
      "         1.0       0.56      0.73      0.63       253\n",
      "\n",
      "    accuracy                           0.63       570\n",
      "   macro avg       0.64      0.64      0.63       570\n",
      "weighted avg       0.65      0.63      0.63       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_RF = RandomForestClassifier(n_jobs=3, oob_score=True, n_estimators=100, criterion=\"gini\")\n",
    "model_RF, y_pred = train_model(model_RF, X_train_imputed, y_train, X_test_imputed, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted Successfully.\n",
      "\u001b[1mROC-AUC Score\u001b[0m \t\t: 60.0 %\n",
      "\u001b[1mBest Threshold\u001b[0m \t\t: 0.507 \n",
      "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.586\n",
      "\u001b[1mModel Accuracy\u001b[0m \t\t: 56.99999999999999 %\n",
      "\u001b[1m\n",
      "Classification Report:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.53      0.58       317\n",
      "         1.0       0.52      0.62      0.57       253\n",
      "\n",
      "    accuracy                           0.57       570\n",
      "   macro avg       0.58      0.58      0.57       570\n",
      "weighted avg       0.59      0.57      0.57       570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_SVM = SVC()\n",
    "model_SVM, y_pred = train_model(model_LR, X_train_imputed, y_train, X_test_imputed, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted Successfully.\n",
      "\u001b[1mROC-AUC Score\u001b[0m \t\t: 51.0 %\n",
      "\u001b[1mBest Threshold\u001b[0m \t\t: 1.000 \n",
      "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.511\n",
      "\u001b[1mModel Accuracy\u001b[0m \t\t: 56.00000000000001 %\n",
      "\u001b[1m\n",
      "Classification Report:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      1.00      0.71       317\n",
      "         1.0       0.00      0.00      0.00       253\n",
      "\n",
      "    accuracy                           0.56       570\n",
      "   macro avg       0.28      0.50      0.36       570\n",
      "weighted avg       0.31      0.56      0.40       570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "model_mlp = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "model_mlp, y_pred = train_model(model_mlp, X_train_imputed, y_train, X_test_imputed, y_test, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2012_increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_target_column = \"2012_increase\"\n",
    "integrated_df = pd.read_csv(os.path.join(f\"../output/bees/training\",f\"full-{current_target_column}-bees.csv\"))\n",
    "X_total, y_total = integrated_df.iloc[:, 3:], integrated_df.iloc[:, 0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, test_size=0.2, random_state=42)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted Successfully.\n",
      "\u001b[1mROC-AUC Score\u001b[0m \t\t: 56.99999999999999 %\n",
      "\u001b[1mBest Threshold\u001b[0m \t\t: 0.609 \n",
      "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.572\n",
      "\u001b[1mModel Accuracy\u001b[0m \t\t: 60.0 %\n",
      "\u001b[1m\n",
      "Classification Report:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.37      0.14      0.20       207\n",
      "         1.0       0.64      0.87      0.73       363\n",
      "\n",
      "    accuracy                           0.60       570\n",
      "   macro avg       0.51      0.50      0.47       570\n",
      "weighted avg       0.54      0.60      0.54       570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_LR = LogisticRegression(max_iter=10000)\n",
    "model_LR, y_pred = train_model(model_LR, X_train_imputed, y_train, X_test_imputed, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted Successfully.\n",
      "\u001b[1mROC-AUC Score\u001b[0m \t\t: 57.99999999999999 %\n",
      "\u001b[1mBest Threshold\u001b[0m \t\t: 0.620 \n",
      "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.563\n",
      "\u001b[1mModel Accuracy\u001b[0m \t\t: 60.0 %\n",
      "\u001b[1m\n",
      "Classification Report:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.22      0.28       207\n",
      "         1.0       0.65      0.82      0.72       363\n",
      "\n",
      "    accuracy                           0.60       570\n",
      "   macro avg       0.52      0.52      0.50       570\n",
      "weighted avg       0.56      0.60      0.56       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_RF = RandomForestClassifier(n_jobs=3, oob_score=True, n_estimators=100, criterion=\"gini\")\n",
    "model_RF, y_pred = train_model(model_RF, X_train_imputed, y_train, X_test_imputed, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted Successfully.\n",
      "\u001b[1mROC-AUC Score\u001b[0m \t\t: 56.99999999999999 %\n",
      "\u001b[1mBest Threshold\u001b[0m \t\t: 0.609 \n",
      "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.572\n",
      "\u001b[1mModel Accuracy\u001b[0m \t\t: 60.0 %\n",
      "\u001b[1m\n",
      "Classification Report:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.37      0.14      0.20       207\n",
      "         1.0       0.64      0.87      0.73       363\n",
      "\n",
      "    accuracy                           0.60       570\n",
      "   macro avg       0.51      0.50      0.47       570\n",
      "weighted avg       0.54      0.60      0.54       570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_SVM = SVC()\n",
    "model_SVM, y_pred = train_model(model_LR, X_train_imputed, y_train, X_test_imputed, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted Successfully.\n",
      "\u001b[1mROC-AUC Score\u001b[0m \t\t: 54.0 %\n",
      "\u001b[1mBest Threshold\u001b[0m \t\t: 1.000 \n",
      "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.526\n",
      "\u001b[1mModel Accuracy\u001b[0m \t\t: 59.0 %\n",
      "\u001b[1m\n",
      "Classification Report:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.37      0.39       207\n",
      "         1.0       0.66      0.71      0.69       363\n",
      "\n",
      "    accuracy                           0.59       570\n",
      "   macro avg       0.54      0.54      0.54       570\n",
      "weighted avg       0.58      0.59      0.58       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "model_mlp = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "model_mlp, y_pred = train_model(model_mlp, X_train_imputed, y_train, X_test_imputed, y_test, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2007_increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_target_column = \"2007_increase\"\n",
    "integrated_df = pd.read_csv(os.path.join(f\"../output/bees/training\",f\"full-{current_target_column}-bees.csv\"))\n",
    "X_total, y_total = integrated_df.iloc[:, 3:], integrated_df.iloc[:, 0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, test_size=0.2, random_state=42)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted Successfully.\n",
      "\u001b[1mROC-AUC Score\u001b[0m \t\t: 56.99999999999999 %\n",
      "\u001b[1mBest Threshold\u001b[0m \t\t: 0.612 \n",
      "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.559\n",
      "\u001b[1mModel Accuracy\u001b[0m \t\t: 61.0 %\n",
      "\u001b[1m\n",
      "Classification Report:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.11      0.17       209\n",
      "         1.0       0.64      0.90      0.75       361\n",
      "\n",
      "    accuracy                           0.61       570\n",
      "   macro avg       0.52      0.51      0.46       570\n",
      "weighted avg       0.55      0.61      0.54       570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_LR = LogisticRegression(max_iter=10000)\n",
    "model_LR, y_pred = train_model(model_LR, X_train_imputed, y_train, X_test_imputed, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted Successfully.\n",
      "\u001b[1mROC-AUC Score\u001b[0m \t\t: 56.00000000000001 %\n",
      "\u001b[1mBest Threshold\u001b[0m \t\t: 0.630 \n",
      "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.545\n",
      "\u001b[1mModel Accuracy\u001b[0m \t\t: 61.0 %\n",
      "\u001b[1m\n",
      "Classification Report:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.16      0.23       209\n",
      "         1.0       0.64      0.87      0.74       361\n",
      "\n",
      "    accuracy                           0.61       570\n",
      "   macro avg       0.53      0.52      0.49       570\n",
      "weighted avg       0.56      0.61      0.55       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_RF = RandomForestClassifier(n_jobs=3, oob_score=True, n_estimators=100, criterion=\"gini\")\n",
    "model_RF, y_pred = train_model(model_RF, X_train_imputed, y_train, X_test_imputed, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted Successfully.\n",
      "\u001b[1mROC-AUC Score\u001b[0m \t\t: 56.99999999999999 %\n",
      "\u001b[1mBest Threshold\u001b[0m \t\t: 0.612 \n",
      "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.559\n",
      "\u001b[1mModel Accuracy\u001b[0m \t\t: 61.0 %\n",
      "\u001b[1m\n",
      "Classification Report:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.11      0.17       209\n",
      "         1.0       0.64      0.90      0.75       361\n",
      "\n",
      "    accuracy                           0.61       570\n",
      "   macro avg       0.52      0.51      0.46       570\n",
      "weighted avg       0.55      0.61      0.54       570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_SVM = SVC()\n",
    "model_SVM, y_pred = train_model(model_LR, X_train_imputed, y_train, X_test_imputed, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted Successfully.\n",
      "\u001b[1mROC-AUC Score\u001b[0m \t\t: 53.0 %\n",
      "\u001b[1mBest Threshold\u001b[0m \t\t: 0.000 \n",
      "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.536\n",
      "\u001b[1mModel Accuracy\u001b[0m \t\t: 55.00000000000001 %\n",
      "\u001b[1m\n",
      "Classification Report:\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.44      0.42       209\n",
      "         1.0       0.66      0.62      0.64       361\n",
      "\n",
      "    accuracy                           0.55       570\n",
      "   macro avg       0.53      0.53      0.53       570\n",
      "weighted avg       0.56      0.55      0.56       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "model_mlp = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "model_mlp, y_pred = train_model(model_mlp, X_train_imputed, y_train, X_test_imputed, y_test, False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
